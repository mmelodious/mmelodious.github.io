---
layout: post
title: An Unsupervised Neural Attention Model for Aspect Extraction 阅读笔记
date: 2018-03-14 
tags: aspect-sentiment-analysis    
---

　　An Unsupervised Neural Attention Model for Aspect Extraction [论文地址](http://www.aclweb.org/anthology/P/P17/P17-1036.pdf)。此篇是ACL2017的一篇文章，代码地址在[这里](https://github.com/ruidan/Unsupervised-Aspect-Extraction)

### 目的:
　　基于aspect的情感分析是互联网产品评论分析的工作之一，但由于aspect的标注工作量巨大，所以很自然的会考虑使用无监督的方式来处理手上已有的数据。最近抓取到了一些Appstore及Google Play的app评论数据,但是这些数据并没有对应的aspect的标签，所以选用了这篇无监督的方法进行实验。
　　
### 本文使用的方法:
　　摘要：The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings.此句提到了使用单词的共现来提高所提取aspect的一致性。单词嵌入模型鼓励出现在相似上下文中的单词在嵌入空间中彼此接近。另外，使用注意机制在训练过程中不重视不相关的词汇，进一步提高各aspect的一致性。
<br>
　　尽管基于LDA模型发现的各个方面可能会很好地描述一个语料库，但我们发现推测出的各个aspect质量差 - 通常由无关或松散相关的概念组成。这可能会大大降低用户对使用这种自动化系统的信心。质量差可能有两个主要原因：传统的LDA模型不直接对单词共现统计进行编码，这是统计信息的主要来源，以保持主题一致性。LDA假设每个单词都是独立生成的，通过从文档级建模文字来隐式捕获这些模式。此外，基于LDA的模型需要估计每个文档的主题分布。
<br>
　　训练目标函数：最小化重构误差。Hinge Loss是一种目标函数（或者说损失函数）的名称，有的时候又叫做max-margin objective。其最著名的应用是作为SVM的目标函数。实际应用中，一方面很多时候我们的y的值域并不是[-1,1]，比如我们可能更希望y更接近于一个概率，即其值域最好是[0,1]。另一方面，很多时候我们希望训练的是两个样本之间的相似关系，而非样本的整体分类，所以很多时候我们会用下面的公式：
<br>
　　l(y,y′)=max(0,m−y+y′)
<br>
　　其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数）即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。比如，我们想训练词向量，我们希望经常同时出现的词，他们的向量内积越大越好；不经常同时出现的词，他们的向量内积越小越好。
<br>
　　本文的目标是使重建的嵌入rs与目标语句嵌入zs类似，而与那些负面的样本不同。正则化: 方面嵌入矩阵T在训练期间可能遭受冗余问题,为了确保得到的方面嵌入的多样性，为目标函数J添加正则化项以鼓励每个方面嵌入的唯一性,U(θ) = || T n · Tn' − I ||,其中I为单位矩阵，正则化部分鼓励方面嵌入矩阵T的行之间的正交性并且惩罚不同方面向量之间的冗余。 最终目标函数L是通过添加J和U得到。
	

　　
　　
　　
### 实验记录:
　　github中提到使用theano==0.9.0，但由于服务器安装的是cuda8.0版本，会报"cudnnSetFilterNdDescriptor_v4" is undefined的错误，使用了theano==1.0.1解决了此问题。
<br>
　　通过删除标点符号，停用词和出现少于10次的词来预处理语料库。btm使用了[这份代码](http://code.google.com/p/btm/0).文中使用了[Gibbs Simples](https://www.jianshu.com/p/27829d842fbc)
　　



　　
　　





<br>
转载请注明：[hbzou的博客](http://hbzou.com) » [点击阅读原文]()     













